# Azure-Data-Engineering-Project
**End-to-End Data Pipeline Implementation Using Azure**

---

## **Project Overview**  
This project demonstrates an end-to-end data engineering solution built using Microsoft Azure for healthcare revenue cycle management domain. The pipeline automates data ingestion, transformation, and storage, providing an efficient and scalable system for managing enterprise data.  

## **Project Architecture**  
![logo](https://github.com/codeSavvy-ln/Azure-Data-Pipeline-Project/blob/main/Images/project%20architecture%20snapshot.png)

---

## **Project Structure**
```
â”œâ”€â”€ Azure SQL database
    â”œâ”€â”€ hospital-a
    â”‚   â”œâ”€â”€ departments.csv
    â”‚   â”œâ”€â”€ encounters.csv
    â”‚   â”œâ”€â”€ hospital_a
    â”‚   â”œâ”€â”€ patients.csv
    â”‚   â”œâ”€â”€ providers.csv
    â”‚   â”œâ”€â”€ readme
    â”‚   â””â”€â”€ transactions.csv
    â”œâ”€â”€ hospital-b
    â”‚   â”œâ”€â”€ departments.csv
    â”‚   â”œâ”€â”€ encounters.csv
    â”‚   â”œâ”€â”€ hospital_b
    â”‚   â”œâ”€â”€ patients.csv
    â”‚   â”œâ”€â”€ providers.csv
    â”‚   â”œâ”€â”€ readme
    â”‚   â””â”€â”€ transactions.csv
    â””â”€â”€ readme
â”œâ”€â”€ README.md
â”œâ”€â”€ dataset
    â”œâ”€â”€ AzureDatabricksDeltaLakeDataset1.json
    â”œâ”€â”€ generic_adls_flat_file_ds.json
    â”œâ”€â”€ generic_adls_parquet_ds.json
    â””â”€â”€ generic_sql_ds.json
â”œâ”€â”€ factory
    â””â”€â”€ ADF-HC-RCM-DEV-Project.json
â”œâ”€â”€ landing
    â”œâ”€â”€ CPT codes
    â”‚   â”œâ”€â”€ cptcodes.csv
    â”‚   â””â”€â”€ readme
    â”œâ”€â”€ claims
    â”‚   â”œâ”€â”€ hospital1_claim_data.csv
    â”‚   â”œâ”€â”€ hospital2_claim_data.csv
    â”‚   â””â”€â”€ readme
    â””â”€â”€ readme
â”œâ”€â”€ linkedService
    â”œâ”€â”€ AzureDataLakeStorage1.json
    â”œâ”€â”€ AzureDatabricks1.json
    â”œâ”€â”€ AzureDatabricksDeltaLake1_ls.json
    â”œâ”€â”€ AzureKeyVault_ls.json
    â””â”€â”€ hosa_sql_ls.json
â”œâ”€â”€ load_config.csv
â”œâ”€â”€ notebook
    â”œâ”€â”€ API
    â”‚   â”œâ”€â”€ ICD Code API extract.ipynb
    â”‚   â”œâ”€â”€ NPI API extract.ipynb
    â”‚   â””â”€â”€ readme
    â”œâ”€â”€ gold
    â”‚   â”œâ”€â”€ dim_cpt_code.py
    â”‚   â”œâ”€â”€ dim_department.py
    â”‚   â”œâ”€â”€ dim_icd_code.ipynb
    â”‚   â”œâ”€â”€ dim_npi.ipynb
    â”‚   â”œâ”€â”€ dim_patient.py
    â”‚   â”œâ”€â”€ dim_provider.py
    â”‚   â”œâ”€â”€ fact_transaction.sql
    â”‚   â””â”€â”€ readme
    â”œâ”€â”€ readme
    â”œâ”€â”€ set up
    â”‚   â”œâ”€â”€ ADLS_mount.py
    â”‚   â”œâ”€â”€ Audit_table.py
    â”‚   â””â”€â”€ readme
    â””â”€â”€ silver
    â”‚   â”œâ”€â”€ CPT codes.py
    â”‚   â”œâ”€â”€ Claims.py
    â”‚   â”œâ”€â”€ Departments_F.py
    â”‚   â”œâ”€â”€ Encounters.py
    â”‚   â”œâ”€â”€ ICD Code.ipynb
    â”‚   â”œâ”€â”€ NPI.ipynb
    â”‚   â”œâ”€â”€ Patient.py
    â”‚   â”œâ”€â”€ Providers_F.py
    â”‚   â”œâ”€â”€ Transactions.py
    â”‚   â””â”€â”€ readme
â”œâ”€â”€ pipeline
    â”œâ”€â”€ emr_pl_src_to_landing.json
    â”œâ”€â”€ silver_to_gold.json
â””â”€â”€ publish_config.json
```

## **Technologies Used**
- **Programming Language:** Python, SQL, Pyspark
- **Azure Data Factory**
- **Azure Databricks**
- **Azure Key Vault**
- **Data Sources:**
    - **Azure SQL Database**
    - **Azure storage:** ADLS Gen2, Delta Lake.
    - **API:** ICD code, NPI.

---


## **Pipeline Architecture (ETL Pipeline Breakdown)**  

### **Step 1: Setting Up the Azure Environment** âš™ï¸

To start, the following Azure resources were provisioned:

- **Azure SQL database:** To store EMR related data in multiple tables under 2 databases (hospital-a,hospital-b)
- **Azure Data Factory (ADF):** Used for data orchestration and automation.
- **Azure Storage Account:** Acts as the data lake, storing raw (bronze), transformed (silver), and curated (gold) data.
- **Azure Databricks:** Performs data transformations and implementing SCD Type2 and Common data management.
- **Key Vault:** To secure all the keys.

### **Step 2: Implementing the Data Pipeline Using ADF** ğŸš€

**Azure Data Factory (ADF)** serves as the backbone for orchestrating the data pipeline.

[load_config](https://github.com/codeSavvy-ln/Azure-Data-Pipeline-Project/blob/main/load_config.csv) is the main file that holds config related details which will be referenced to implement the Data pipeline.

**Pipeline 1 - emr_pl_src_to_landing**

  - **Lookup Activity** is used to retrieve the data from load_config file. 
  - **for each activity** will iterate over each row in load_config file and perform the opertions(activity) on it.
  
![logo](https://github.com/codeSavvy-ln/Azure-Data-Pipeline-Project/blob/main/Images/emr_pl_src_to_landing.png)

**Within for each activity**

![logo](https://github.com/codeSavvy-ln/Azure-Data-Pipeline-Project/blob/main/Images/for_each.png)

   - **Get Metadata Activity** checks if the targetpath and tablename from load_config exists in bronze container.
   - **If Condition** If the output of get metadata activity is true then copy activity is ran to copy the data into archive folder under current date format yyyy/MM/DD.
   - **If Condition (Fetch_Logs)** checks if the loadtype is full
     - **If loadtype = Full** takes the data from SQL DB and store it into bronze container in parquet format and also stores the pipeline running details in the audit table
     - **If loadtype <> Full** fetches the latest loaddate and compare it in the next increamental load activity which copies the data from SQL db into bronze layer in the form of parquet format and then further update the audit table with the latest data.


**Pipeline 2 - silver_to_gold**
![logo](https://github.com/codeSavvy-ln/Azure-Data-Pipeline-Project/blob/main/Images/silver_to_gold.png)

 All the code for silver and gold layer stored in databricks notebook can be triggered from ADF itself.
- **silver** Data is in Delta format here, Data loaded here will be cleaned and SCD type 2 will also be implemented and then this will be sent to the gold layer.
- **gold** - Data is in Delta format here , Fact and Dimension table are implemented here.

### **Step 3: Data Transformation with Azure Databricks** ğŸ”„

Using Azure Databricks, the raw data from the bronze container was transformed into a structured format.

- **Audit Table** logs the ADF pipeline details like data_source, tablename, number of rows copied and load date once the pipeline run successfully

---



---

## **How to Use**  

### **1. Prerequisites**  
- Active Azure Subscription.  
- Azure Databricks cluster with below specs:
  - Databricks runtime: 15.4 LTS(includes Apache Spark 3.5.0, Scala 2.12)
  - Worker Type: Standard_D2ds_v6 - min:1 - Max:1 - Current:0  

### **2. Steps to Set Up**  
1. Clone this repository:  
   ```bash
   git clone https://github.com/codeSavvy-ln/Azure-Data-Pipeline-Project.git
